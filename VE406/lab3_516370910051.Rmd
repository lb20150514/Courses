---
title: 
  "Lab 3"
author: 
  "Ve406"
date: 
  'Due: __5 November 2018, 11:40am__'
header-inclues:
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage{amsthm}  
  - \usepackage{amsthm}  
  - \usepackage{listings}
output: 
  pdf_document:
  fig_width: 6
  fig_height: 6
---

-----

# Task 1 (8 points)

## (a) (1 point)

Succesfully render this file. 

## (b) (1 point)

```{r}
chem_pro.df = read.table("chem_pro.csv", sep = ",", header = TRUE)
```

+ Clean 'ratio'

```{r}
ratio_typo = which(chem_pro.df$ratio == "0>163")
chem_pro.df$ratio = as.character(chem_pro.df$ratio)
chem_pro.df$ratio[ratio_typo] = "0.163"
chem_pro.df$ratio = as.double(chem_pro.df$ratio)
```

+ Clean 'conversion'

```{r}
conversion_typo = which(chem_pro.df$conversion <= -10)
chem_pro.df$conversion[conversion_typo] = - chem_pro.df$conversion[conversion_typo]
```


## (c) (1 point)

Refer to '? pairs' we get 'panel.hist' and 'panel.cor'.
```{r}
panel.hist <- function(x, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}
```

Then

```{r}
pairs(chem_pro.df, upper.panel = panel.smooth, diag.panel = panel.hist, lower.panel = panel.cor)
```

## (d) (1 point)

```{r chem_pro.lm, results = 'hide'}
chem_pro.LM = lm(yield~conversion+flow+ratio, data = chem_pro.df)
```

+ Standardised residual Vs fitted value 

```{r fig.align="center", out.width="50%", out.height='50%'}
plot(fitted.values(chem_pro.LM), rstandard(chem_pro.LM), xlab = "Fitted value", ylab = "Standardised residual")
abline(a = 0, b = 0, lty = 2, col = "blue")
```

+ Standardised residual Vs conversion

```{r fig.align="center", out.width="50%", out.height='50%'}
plot(chem_pro.df$conversion, rstandard(chem_pro.LM), xlab = "Conversion", ylab = "Standardised residual")
abline(a = 0, b = 0, lty = 2, col = "blue")
```

+ Standardised residual Vs flow

```{r fig.align="center", out.width="50%", out.height='50%'}
plot(chem_pro.df$flow, rstandard(chem_pro.LM), xlab = "Flow", ylab = "Standardised residual")
abline(a = 0, b = 0, lty = 2, col = "blue")
```

+ Standardised residual Vs ratio

```{r fig.align="center", out.width="50%", out.height='50%'}
plot(chem_pro.df$ratio, rstandard(chem_pro.LM), xlab = "Ratio", ylab = "Standardised residual")
abline(a = 0, b = 0, lty = 2, col = "blue")
```

+ Residual Vs Previous Residual

```{r fig.align="center", out.width="50%", out.height='50%'}
plot(chem_pro.LM$residuals[-length(chem_pro.LM$residuals)], chem_pro.LM$residuals[-1], xlab = "Previous", ylab = "Residual")
abline(a = 0, b = 0, lty = 2, col = "blue")
```

+ Residual Autcorrelation (ACF)

```{r fig.align="center", out.width="50%", out.height='50%'}
acf(chem_pro.LM$residuals)
```

+ Q-Q Normal

```{r fig.align="center", out.width="50%", out.height='50%'}
qqnorm(chem_pro.LM$residuals)
qqline(chem_pro.LM$residuals)
```


## (e) (1 point)

```{r}
VIF <- rep(0,3)
names(VIF) <- c("conversion", "flow", "ratio")
chem_pro.conversion.LM = lm(conversion ~ flow + ratio, data = chem_pro.df)
VIF[1] <- 1 / (1 - summary(chem_pro.conversion.LM)$r.squared)
chem_pro.flow.LM = lm(flow ~ conversion + ratio, data = chem_pro.df)
VIF[2] <- 1 / (1 - summary(chem_pro.flow.LM)$r.squared)
chem_pro.ratio.LM = lm(ratio ~ conversion + flow, data = chem_pro.df)
VIF[3] <- 1 / (1 - summary(chem_pro.ratio.LM)$r.squared)
VIF
```

## (f) (1 point)

```{r fig.align="center", out.width="50%", out.height='50%'}
pii.vec = hatvalues(chem_pro.LM)
boxplot(pii.vec, xlab = "chem_pro.LM", ylab = "Leverage Scores", ylim = c(0.05, 0.3))
excessive <- 3 * (3 + 1) / 44
abline(a = mean(pii.vec), b = 0, lty = 2, col = "blue")
abline(a = excessive, b = 0, lty = 2, col = "red")
```

## (g) (1 point)

```{r fig.align="center", out.width="50%", out.height='50%'}
plot(pii.vec, rstandard(chem_pro.LM), col = "white", xlab = "Leverage Score", ylab = "Standardised Residuals")
text(x = pii.vec, y = rstandard(chem_pro.LM), c(1:44))
```

## (h) (1 point)

```{r}
im = influence.measures(chem_pro.LM)
im
```

It is suggested that we should delete influential points 6, 7, 16, 35.

# Task 2 (6 points)

## (a) (1 point) 

```{r first model, results = 'hide'}
usare.df = read.table("USA_real_estate.txt", sep = "", header = TRUE)
usare.LM = lm(mppsf~pnh+pms, data = usare.df)
```

```{r fig.align="center", out.width="50%", out.height='50%'}
plot(fitted.values(usare.LM), rstandard(usare.LM), xlab = "Fitted value", ylab = "Standardised Residuals")
abline(a = 0, b = 0, lty = 2, col = "red")
qqnorm(usare.LM$residuals)
qqline(usare.LM$residuals)
```

From the first plot we can see that with the increase of 'fitted.value' in 'usare.LM', the standardised residual increases. And the Q-Q plot proved that the residual does not follow a normal distribution. Therefore, there does exist heteroskedasticity.

## (b) (1 point)

```{r}
z = 2 * log(abs(usare.LM$residuals))
auxiliary.LM = lm(z ~ pnh + pms, data = usare.df)
w.vec = 1 / exp(auxiliary.LM$fitted.values)
```


## (c) (1 point)

```{r}
usare.WLS = lm(mppsf ~ pnh + pms, weights = w.vec, data = usare.df)
summary(usare.WLS)
```


## (d) (1 point)

There is no regulation on choosing weight. The only principle is that our choice should be related to the variables in the model. According to the meaning of 'ns', *Number Homes from which the Median Price is computed*, it is one of the original data source to build 'mppsf'. Therefore, we can use it as weight.


## (e) (1 point)

```{r}
usare.ns.WLS = lm(mppsf ~ pnh + pms, weights = ns, data = usare.df)
summary(usare.ns.WLS)
```


## (f) (1 point)

```{r}
split.screen(c(2,2))
screen(1)
plot(fitted.values(usare.WLS), rstandard(usare.WLS), ylab = "std residuals")
abline(a = 0, b = 0, lty = 2, col = "red")
screen(2)
plot(fitted.values(usare.ns.WLS), rstandard(usare.ns.WLS), ylab = "std residuals")
abline(a = 0, b = 0, lty = 2, col = "blue")
screen(3)
qqnorm(usare.WLS$residuals)
qqline(usare.WLS$residuals, col = "red")
screen(4)
qqnorm(usare.ns.WLS$residuals)
qqline(usare.ns.WLS$residuals, col = "blue")
```

According to the plot result, there is no obvious change in Q-Q improvement. For "standardised residuals v.s. fitted value", we can see that 'usare.WLS' makes some improvement. Therefore, I would prefer the 'usare.WLS' model.

# Task 3

## (a) (1 point)

```{r}
gbo.df = read.table("grossboxoffice.txt", header = T)
gbo.LM = lm(GrossBoxOffice ~ year, data = gbo.df)
summary(gbo.LM)
```

First we look at F-statistic. Since p-value is less than 0.05, we can reject null hypothesis and accept this model.
Then we check R-squared. It indicates that this model can explain the 92.97% of variability in yield.
Finally, we check the p-value of T-test for 'year' and '(Intercept)'. They are both much less than 0.05, so we can use these two coefficients.

## (b) (1 point)

First we plot 'acf' to get a general idea about lag.

```{r fig.align="center", out.width="50%", out.height='50%'}
acf(gbo.LM$residuals)
```

It seems that we need to analyze 'AR(1)', 'AR(2)', 'AR(3)' in detailed.

+ 'AR(1)'

```{r}
gbo.lag1.df = data.frame(GrossBoxOffice = gbo.df$GrossBoxOffice[-32], year = gbo.df$year[-32], year.lag.1 = gbo.df$year[-1])
gbo.AR1 = lm(GrossBoxOffice ~ year + year.lag.1, data = gbo.lag1.df)
summary(gbo.AR1)
```

We should notice that in summary of 'gbo.AR1', the estimate of 'year.lag.1' is 'NA'. It means that the effect of 'year.lag.1' on 'GrossBoxOffice' is exactly the same as that of 'year'. Therefore, we can continue our exploration on 'AR(2)' and 'AR(3)'.

+ 'AR(2)'

```{r}
gbo.lag2.df = data.frame(GrossBoxOffice = gbo.lag1.df$GrossBoxOffice[-31], year = gbo.lag1.df$year[-31], year.lag.1 = gbo.lag1.df$year.lag.1[-31], year.lag.2 = gbo.lag1.df$year.lag.1[-1])
gbo.AR2 = lm(GrossBoxOffice ~ year + year.lag.1 + year.lag.2, data = gbo.lag2.df)
```


## (c) (1 point)

Obtain a final model for predicting `GrossBoxOffice` for `year=1975`, name it as `gbo.final.M`.

## (d) (1 point)

Produce diagnostic plots to justify your choice of model. 

## (e) (1 point)

Describe any weakness in your `gbo.final.M`. 

## (f) (1 point)

Use your model `gbo.final.M` to identify any outliers. 
