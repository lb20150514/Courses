---
title: 
  "Lab 3"
author: 
  "Ve406"
date: 
  'Due: __5 November 2018, 11:40am__'
header-inclues:
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage{amsthm}  
  - \usepackage{amsthm}  
  - \usepackage{listings}
output: 
  pdf_document:
  fig_width: 6
  fig_height: 6
---

-----

# Task 1 (8 points)

## (a) (1 point)

Succesfully render this file. 

## (b) (1 point)

```{r}
chem_pro.df = read.table("chem_pro.csv", sep = ",", header = TRUE)
```

+ Clean 'ratio'

```{r}
ratio_typo = which(chem_pro.df$ratio == "0>163")
chem_pro.df$ratio = as.character(chem_pro.df$ratio)
chem_pro.df$ratio[ratio_typo] = "0.163"
chem_pro.df$ratio = as.double(chem_pro.df$ratio)
```

+ Clean 'conversion'

```{r}
conversion_typo = which(chem_pro.df$conversion <= -10)
chem_pro.df$conversion[conversion_typo] = - chem_pro.df$conversion[conversion_typo]
```


## (c) (1 point)

Refer to '? pairs' we get 'panel.hist' and 'panel.cor'.
```{r}
panel.hist <- function(x, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}
```

Then

```{r}
pairs(chem_pro.df, upper.panel = panel.smooth, diag.panel = panel.hist, lower.panel = panel.cor)
```

## (d) (1 point)

```{r chem_pro.lm, results = 'hide'}
chem_pro.LM = lm(yield~conversion+flow+ratio, data = chem_pro.df)
```

+ Standardised residual Vs fitted value 

```{r}
plot(fitted.values(chem_pro.LM), rstandard(chem_pro.LM), xlab = "Fitted value", ylab = "Standardised residual")
abline(a = 0, b = 0, lty = 2, col = "blue")
```

+ Standardised residual Vs conversion

```{r}
plot(chem_pro.df$conversion, rstandard(chem_pro.LM), xlab = "Conversion", ylab = "Standardised residual")
abline(a = 0, b = 0, lty = 2, col = "blue")
```

+ Standardised residual Vs flow

```{r}
plot(chem_pro.df$flow, rstandard(chem_pro.LM), xlab = "Flow", ylab = "Standardised residual")
abline(a = 0, b = 0, lty = 2, col = "blue")
```

+ Standardised residual Vs ratio

```{r}
plot(chem_pro.df$ratio, rstandard(chem_pro.LM), xlab = "Ratio", ylab = "Standardised residual")
abline(a = 0, b = 0, lty = 2, col = "blue")
```

+ Residual Vs Previous Residual

```{r}
plot(chem_pro.LM$residuals[-length(chem_pro.LM$residuals)], chem_pro.LM$residuals[-1], xlab = "Previous", ylab = "Residual")
abline(a = 0, b = 0, lty = 2, col = "blue")
```

+ Residual Autcorrelation (ACF)

```{r}
acf(chem_pro.LM$residuals)
```

+ Q-Q Normal

```{r}
qqnorm(chem_pro.LM$residuals)
qqline(chem_pro.LM$residuals)
```


## (e) (1 point)

```{r}
VIF <- rep(0,4)
names(VIF) <- c("yield", "conversion", "flow", "ratio")
VIF[1] <- 1 / (1 - summary(chem_pro.LM)$r.squared)
chem_pro.conversion.LM = lm(conversion ~., data = chem_pro.df)
VIF[2] <- 1 / (1 - summary(chem_pro.conversion.LM)$r.squared)
chem_pro.flow.LM = lm(flow ~., data = chem_pro.df)
VIF[3] <- 1 / (1 - summary(chem_pro.flow.LM)$r.squared)
chem_pro.ratio.LM = lm(ratio ~., data = chem_pro.df)
VIF[4] <- 1 / (1 - summary(chem_pro.ratio.LM)$r.squared)
VIF
```

## (f) (1 point)

```{r}
pii.vec = hatvalues(chem_pro.LM)
boxplot(pii.vec, xlab = "chem_pro.LM", ylab = "Leverage Scores", ylim = c(0.05, 0.3))
excessive <- 3 * (3 + 1) / 44
abline(a = mean(pii.vec), b = 0, lty = 2, col = "blue")
abline(a = excessive, b = 0, lty = 2, col = "red")
```

## (g) (1 point)

```{r}
plot(pii.vec, rstandard(chem_pro.LM), col = "white", xlab = "Leverage Score", ylab = "Standardised Residuals")
text(x = pii.vec, y = rstandard(chem_pro.LM), c(1:44))
```

## (h) (1 point)

```{r}
im = influence.measures(chem_pro.LM)
im
```

It is suggested that we should delete influential points 6, 7, 16, 35.

# Task 2 (6 points)

## (a) (1 point) 

```{r first model, results = 'hide'}
usare.df = read.table("USA_real_estate.txt", sep = "", header = TRUE)
usare.LM = lm(mppsf~pnh+pms, data = usare.df)
```

```{r}
plot(fitted.values(usare.LM), rstandard(usare.LM), xlab = "Fitted value", ylab = "Standardised Residuals")
```
It is obvious in this plot that with the increase of 'fitted.value' in 'usare.LM', the standardised residual increases. Therefore, there does exist heteroskedasticity.

## (b) (1 point)

```{r}
factors.group = factor(usare.df$pnh + usare.df$pms)
convar = tapply(usare.df$mppsf, factors.group, var)
v.vec = convar[factors.group]
w.vec = 1 / v.vec
```


## (c) (1 point)

```{r}
usare.WLS = lm(mppsf ~ pnh + pms, weights = w.vec, data = usare.df)
summary(usare.WLS)
```


## (d) (1 point)




## (e) (1 point)

```{r}
ns.group = factor(usare.df$ns)
convar.ns = tapply(usare.df$mppsf, ns.group, var)
w.ns.vec = 1 / convar.ns[ns.group]
usare.ns.WLS = lm(mppsf ~ pnh + pms, weights = w.ns.vec, data = usare.df)
summary(usare.ns.WLS)
```


## (f) (1 point)

Compare `usare.WLS` with `usare.ns.WLS`. Which of the two models do you prefer? Explain your answer. 

# Task 3

## (a) (1 point)

```{r}
gbo.df = read.table("grossboxoffice.txt", header = T)
gbo.LM = lm(GrossBoxOffice ~ year, data = gbo.df)
summary(gbo.LM)
```

First we look at F-statistic. Since p-value is less than 0.05, we can reject null hypothesis and accept this model.
Then we check R-squared. It indicates that this model can explain the 92.97% of variability in yield.
Finally, we check the p-value of T-test for 'year' and '(Intercept)'. They are both much less than 0.05, so we can use these two coefficients.

## (b) (1 point)

First we try to detect the presence of 'AR(1)'.

```{r}
lag = 1
index1 = 0:(lag - 1) - nrow(gbo.df)
year.lag.1 = gbo.df$year[index1]
GrossBoxOffice.1 = gbo.df$GrossBoxOffice[-(1:lag)]
year.1 = gbo.df$year[-(1:lag)]
gbo.lag1.LM = lm(GrossBoxOffice.1 ~ year.1 + year.lag.1)
summary(gbo.lag1.LM)
```

We should notice that in summary of 'gbo.lag1.LM', the estimate of 'year.lag.1' is 'NA'. What's more, there is a line saying *1 not defined because of singularities*, which means that the effect of 'year.lag.1' on 'GrossBoxOffice' is exactly the same as that of 'year' and thus this item can be omitted. Therefore, the possibility of using 'AR(1)' is small.
Since the data size is small, 'AR(2)' and 'AR(3)' should be unnecessary when we don't have a significant 'AR(1)'.

## (c) (1 point)

Obtain a final model for predicting `GrossBoxOffice` for `year=1975`, name it as `gbo.final.M`.

## (d) (1 point)

Produce diagnostic plots to justify your choice of model. 

## (e) (1 point)

Describe any weakness in your `gbo.final.M`. 

## (f) (1 point)

Use your model `gbo.final.M` to identify any outliers. 
