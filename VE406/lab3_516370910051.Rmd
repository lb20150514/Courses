---
title: 
  "Lab 3"
author: 
  "Ve406"
date: 
  'Due: __5 November 2018, 11:40am__'
header-inclues:
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage{amsthm}  
  - \usepackage{amsthm}  
  - \usepackage{listings}
output: 
  pdf_document:
  fig_width: 6
  fig_height: 6
---

-----

# Task 1 (8 points)

## (a) (1 point)

Succesfully render this file. 

## (b) (1 point)

```{r}
chem_pro.df = read.table("chem_pro.csv", sep = ",", header = TRUE)
```

### Clean `ratio`

```{r}
ratio_typo = which(chem_pro.df$ratio == "0>163")
chem_pro.df$ratio = as.character(chem_pro.df$ratio)
chem_pro.df$ratio[ratio_typo] = "0.163"
chem_pro.df$ratio = as.double(chem_pro.df$ratio)
```

### Clean `conversion`

```{r}
conversion_typo = which(chem_pro.df$conversion <= -10)
chem_pro.df$conversion[conversion_typo] = - chem_pro.df$conversion[conversion_typo]
```


## (c) (1 point)

Refer to `? pairs` we get `panel.hist()` and `panel.cor()`.
```{r}
panel.hist <- function(x, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}
```

Then

```{r}
pairs(chem_pro.df, upper.panel = panel.smooth, diag.panel = panel.hist, 
      lower.panel = panel.cor)
```

## (d) (1 point)

```{r chem_pro.lm, results = 'hide'}
chem_pro.LM = lm(yield~conversion+flow+ratio, data = chem_pro.df)
```

+ Standardised residual Vs fitted value 

```{r fig.align="center", out.width="80%", out.height='80%'}
plot(fitted.values(chem_pro.LM), rstandard(chem_pro.LM), xlab = "Fitted value",
     ylab = "Standardised residual")
abline(a = 0, b = 0, lty = 2, col = "blue")
```

+ Standardised residual Vs conversion

```{r fig.align="center", out.width="80%", out.height='80%'}
plot(chem_pro.df$conversion, rstandard(chem_pro.LM), xlab = "Conversion",
     ylab = "Standardised residual")
abline(a = 0, b = 0, lty = 2, col = "blue")
```

+ Standardised residual Vs flow

```{r fig.align="center", out.width="80%", out.height='80%'}
plot(chem_pro.df$flow, rstandard(chem_pro.LM), xlab = "Flow", 
     ylab = "Standardised residual")
abline(a = 0, b = 0, lty = 2, col = "blue")
```

+ Standardised residual Vs ratio

```{r fig.align="center", out.width="80%", out.height='80%'}
plot(chem_pro.df$ratio, rstandard(chem_pro.LM), xlab = "Ratio",  
     ylab = "Standardised residual")
abline(a = 0, b = 0, lty = 2, col = "blue")
```

+ Residual Vs Previous Residual

```{r fig.align="center", out.width="80%", out.height='80%'}
plot(chem_pro.LM$residuals[-length(chem_pro.LM$residuals)], chem_pro.LM$residuals[-1],
     xlab = "Previous", ylab = "Residual")
abline(a = 0, b = 0, lty = 2, col = "blue")
```

+ Residual Autcorrelation (ACF)

```{r fig.align="center", out.width="50%", out.height='50%'}
acf(chem_pro.LM$residuals)
```

+ Q-Q Normal

```{r fig.align="center", out.width="80%", out.height='80%'}
qqnorm(chem_pro.LM$residuals)
qqline(chem_pro.LM$residuals)
```


## (e) (1 point)

```{r}
VIF <- rep(0,3)
names(VIF) <- c("conversion", "flow", "ratio")
chem_pro.conversion.LM = lm(conversion ~ flow + ratio, data = chem_pro.df)
VIF[1] <- 1 / (1 - summary(chem_pro.conversion.LM)$r.squared)
chem_pro.flow.LM = lm(flow ~ conversion + ratio, data = chem_pro.df)
VIF[2] <- 1 / (1 - summary(chem_pro.flow.LM)$r.squared)
chem_pro.ratio.LM = lm(ratio ~ conversion + flow, data = chem_pro.df)
VIF[3] <- 1 / (1 - summary(chem_pro.ratio.LM)$r.squared)
VIF
```

## (f) (1 point)

```{r fig.align="center", out.width="80%", out.height='80%'}
pii.vec = hatvalues(chem_pro.LM)
boxplot(pii.vec, xlab = "chem_pro.LM", ylab = "Leverage Scores", ylim = c(0.05, 0.3))
excessive <- 3 * (3 + 1) / 44
abline(a = mean(pii.vec), b = 0, lty = 2, col = "blue")
abline(a = excessive, b = 0, lty = 2, col = "red")
```

## (g) (1 point)

```{r fig.align="center", out.width="80%", out.height='80%'}
plot(pii.vec, rstandard(chem_pro.LM), col = "white", xlab = "Leverage Score",
     ylab = "Standardised Residuals")
text(x = pii.vec, y = rstandard(chem_pro.LM), c(1:44))
```

## (h) (1 point)

```{r}
im = influence.measures(chem_pro.LM)
im
```

It is suggested that we should delete influential points 6, 7, 16, 35.

# Task 2 (6 points)

## (a) (1 point) 

```{r first model, results = 'hide'}
usare.df = read.table("USA_real_estate.txt", sep = "", header = TRUE)
usare.LM = lm(mppsf~pnh+pms, data = usare.df)
```

```{r fig.align="center", out.width="80%", out.height='80%'}
plot(fitted.values(usare.LM), rstandard(usare.LM), xlab = "Fitted value", 
     ylab = "Standardised Residuals")
abline(a = 0, b = 0, lty = 2, col = "red")
qqnorm(usare.LM$residuals)
qqline(usare.LM$residuals)
```

From the first plot we can see that with the increase of `fitted.value` in `usare.LM`, the standardised residual increases. And the Q-Q plot proved that the residual does not follow a normal distribution. Therefore, there does exist heteroskedasticity.

## (b) (1 point)

```{r}
z = 2 * log(abs(usare.LM$residuals))
auxiliary.LM = lm(z ~ pnh + pms, data = usare.df)
w.vec = 1 / exp(auxiliary.LM$fitted.values)
```


## (c) (1 point)

```{r}
usare.WLS = lm(mppsf ~ pnh + pms, weights = w.vec, data = usare.df)
summary(usare.WLS)
```


## (d) (1 point)

There is no regulation on choosing weight. The only principle is that our choice should be related to the variables in the model. According to the meaning of `ns`, *Number Homes from which the Median Price is computed*, it is one of the original data source to build 'mppsf'. Therefore, we can use it as weight.


## (e) (1 point)

```{r}
usare.ns.WLS = lm(mppsf ~ pnh + pms, weights = ns, data = usare.df)
summary(usare.ns.WLS)
```


## (f) (1 point)

```{r fig.align="center", out.width="80%", out.height='80%', results="hide"}
split.screen(c(2,2))
screen(1)
plot(fitted.values(usare.WLS), rstandard(usare.WLS), ylab = "std residuals")
abline(a = 0, b = 0, lty = 2, col = "red")
screen(2)
plot(fitted.values(usare.ns.WLS), rstandard(usare.ns.WLS), ylab = "std residuals")
abline(a = 0, b = 0, lty = 2, col = "blue")
screen(3)
qqnorm(usare.WLS$residuals)
qqline(usare.WLS$residuals, col = "red")
screen(4)
qqnorm(usare.ns.WLS$residuals)
qqline(usare.ns.WLS$residuals, col = "blue")
```

According to the plot result, there is no obvious change in Q-Q improvement. For "standardised residuals v.s. fitted value", we can see that `usare.WLS` makes some improvement. Therefore, I would prefer the `usare.WLS` model.

# Task 3

## (a) (1 point)

```{r fig.align="center", out.width="80%", out.height='80%'}
gbo.df = read.table("grossboxoffice.txt", header = T)
gbo.LM = lm(GrossBoxOffice ~ year, data = gbo.df)
summary(gbo.LM)
acf(gbo.LM$residuals)
```

According to the acf plot, it is obvious that there exists a correlation between the residuals and its lagged version. Therefore, `gbo.LM` is invalid.

## (b) (1 point)

Since the increasement of year, year.lag.1, year.lag.2, year.lag.3 would be the same, the coefficient of lag terms is too small that even be denoted as `NA` in lag models. A fast estimation is then directly based on `gbo.LM`.

```{r}
res = residuals(gbo.LM)
cor(res[-1], res[-32])
res.lag1.df = data.frame(x = res[-32], y = res[-1])
auxiliary1.LM = lm(y ~ x, data = res.lag1.df)
summary(auxiliary1.LM)
```
It indicates that the residual is positively correlated to previous residual, and F-test of this model shows no obvious evidence against this hypothesis. Therefore, the possibility that we have `AR(1)` is hight.

```{r}
cor(res[-(1:2)], res[-(32:31)])
res.lag2.df = data.frame(x = res[-(32:31)], y = res[-(1:2)])
auxiliary2.LM = lm(y ~ x, data = res.lag2.df)
summary(auxiliary2.LM)
```

In the same way, we find the possibility of taking `AR(2)` is high.

```{r}
cor(res[-(1:3)], res[-(32:30)])
res.lag3.df = data.frame(x = res[-(32:30)], y = res[-(1:3)])
auxiliary3.LM = lm(y ~ x, data = res.lag3.df)
summary(auxiliary3.LM)
```

Since `R-squared` of model including `AR(3)` is too small, the possiblity of taking `AR(3)` is smaller but still non-negligible.

## (c) (1 point)

```{r}
lag.df = data.frame(GrossBoxOffice = gbo.df$GrossBoxOffice[-32], 
                    year = gbo.df$year[-32],
                    lag = gbo.df$GrossBoxOffice[-1])
gbo.final.M = lm(GrossBoxOffice ~ year + poly(lag,2), 
                 data = lag.df)
summary(gbo.final.M)
predict.lm(gbo.final.M, 
           data.frame(year = 1975, lag = lag.df$GrossBoxOffice[1]))
```

## (d) (1 point)

```{r fig.align="center"}
par(mfrow = c(2,2))
plot(gbo.final.M)
```


## (e) (1 point)

There are mainly two problems about my model.
*Reduce degree of freedom*: The original data base is small, taking autoregression item makes usable data even less.
*Singularity of variables*: The lag term is highly correlated to `year`, which means that these vairables has high possibility to be multicolinear. This will reduce the reliability of model.

## (f) (1 point)

The outliers in my model are points 24, 25, 29.